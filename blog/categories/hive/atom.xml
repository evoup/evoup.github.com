<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hive | Evoup`s Blog]]></title>
  <link href="http://evoupsight.com/blog/categories/hive/atom.xml" rel="self"/>
  <link href="http://evoupsight.com/"/>
  <updated>2014-09-17T11:33:40+08:00</updated>
  <id>http://evoupsight.com/</id>
  <author>
    <name><![CDATA[Evoup`s Blog]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[sqoop导出hive数据到mysql]]></title>
    <link href="http://evoupsight.com/blog/2014/09/16/sqoop-dump-to-mysql/"/>
    <updated>2014-09-16T18:30:00+08:00</updated>
    <id>http://evoupsight.com/blog/2014/09/16/sqoop-dump-to-mysql</id>
    <content type="html"><![CDATA[<p>现为apache项目的sqoop是原来是一款hadoop第三方开源模块，通过使用sqoop可以使进行hadoop(hive)的数据与传统关系型数据库（mysql、pgsql）交互。可以把关系型数据库导入hdfs，也可以把hdfs的数据导入关系型数据库。这里我用实例记录的方式讲一下怎么通过它把hive中的数据导入到mysql结果表中。</p>

<!-- more -->


<h3>安装</h3>

<p>yum install sqoop-noarch sqoop-metastore.noarch</p>

<h3>建表</h3>

<p>首先是hive的表结构
<code>sql
use db_work;
CREATE  TABLE table_work(
  item string,
  value string)
PARTITIONED BY (
  dt int,
  tm int)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t'
STORED AS INPUTFORMAT
  'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://db_work/hive/warehouse/db_work.db/table_work'
</code></p>

<p>注意这里使用\t作为字段分隔符，与之后的sqoop保持一致。</p>

<p>mysql表的格式
<code>sql
use testdb;
CREATE TABLE `result` (
  `item` mediumtext,
  `value` mediumtext
) ENGINE=MyISAM DEFAULT CHARSET=utf8;
</code>
注意sqoop导入的字段如果过长，会报错<p><code>com.mysql.jdbc.MysqlDataTruncation: Data truncation: Data too long for column</code><p>，这里使用mediumtext解决这个问题。</p>

<h3>导入</h3>

<p>导入语句
<code>bash
sqoop export -m8 --connect 'jdbc:mysql://'"${mysql_host}"':3306/testdb?useUnicode=true&amp;characterEncoding=utf8' --username ${mysql_user} --password ${mysqlpass} --table result --export-dir /hive/warehouse/db_work.db/table_work/dt=20140916/tm=1213 --input-fields-terminated-by '\t' --input-null-string '\\N' --input-null-non-string '\\N'
</code>
以上语句可以正确地把名为table_work的hive表，以\t为列分隔符，\N为空数据，导入到名为result的mysql表</p>

<h3>其他说明</h3>

<p>其实sqoop每次导出都会生成一个java文件，如果产生了导入错误，是可以人工通过程序干预解决错误的。这需要2个在sqoop命令后加2个参数&mdash;class-name和&mdash;jar-file来完成。</p>

<h3>参考文章</h3>

<p>《sqoop 从 hive 导到mysql遇到的问题》
<a href="http://ju.outofmemory.cn/entry/44398">http://ju.outofmemory.cn/entry/44398</a></p>

<p>《Sqoop1:exportdatatomysql》
<a href="http://www.haogongju.net/art/2639227">http://www.haogongju.net/art/2639227</a></p>

<p>《sqoop 使用心得(sqoop增量倒入)》
<a href="http://www.xuebuyuan.com/1938334.html">http://www.xuebuyuan.com/1938334.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[hiveserver2的maven配置，使用cdh版本]]></title>
    <link href="http://evoupsight.com/blog/2014/08/30/hiveserver2-maven-config/"/>
    <updated>2014-08-30T17:52:00+08:00</updated>
    <id>http://evoupsight.com/blog/2014/08/30/hiveserver2-maven-config</id>
    <content type="html"><![CDATA[<p>hadoop2.0下开发软件我目前主要采用maven做项目构建管理，但要注意要使生产环境的版本和开发的版本一致。可以做的就是先了解生产环境的版本，然后给自己的软件指定相同的版本，避免兼容问题的发生。以下是方法记录。</p>

<!-- more -->


<h4>添加仓库</h4>

<p>首先在pom.xml文件中添加repository
```
<repositories></p>

<pre><code>&lt;repository&gt;
    &lt;id&gt;cloudera&lt;/id&gt;
    &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;
&lt;/repository&gt;
</code></pre>

<p></repositories>
```</p>

<h4>加入依赖</h4>

<p>然后改用cdh的依赖，提供的artifactId和version就可以了
主要是2个包的事情
```
<dependency></p>

<pre><code>&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
&lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;
&lt;version&gt;0.10.0-cdh4.7.0&lt;/version&gt;
</code></pre>

<p></dependency>
<dependency></p>

<pre><code>&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
&lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
&lt;version&gt;2.0.0-cdh4.7.0&lt;/version&gt;
</code></pre>

<p></dependency>
```</p>

<p>依赖的部分，也可以用以下的方式载入
<dependency></p>

<pre><code>        &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;  
        &lt;artifactId&gt;htmlparser&lt;/artifactId&gt;  
        &lt;version&gt;2.0&lt;/version&gt;  
        &lt;scope&gt;system&lt;/scope&gt;  
        &lt;systemPath&gt;${project.basedir}/lib/xxx.jar&lt;/systemPath&gt;  
</code></pre>

<p></dependency></p>

<p>然后是hiveserver2使用和hiveserver1要注意的地方</p>

<p>hiveserver2<p>
<code>java
URLHIVE="jdbc:hive2://"+hive_host+":"+hive_port+"/smartmad;auth=noSasl";
Class.forName("org.apache.hive.jdbc.HiveDriver");
</code>
hiveserver1<p>
<code>java
URLHIVE="jdbc:hive://"+hive_host+":"+hive_port+"/smartmad;auth=noSasl";
Class.forName("org.apache.hadoop.hive.jdbc.HiveDriver");
</code>
这里我们从hive改进hiveserver2的包命名的角度不难看出，该版本已成为apache顶级项目。</p>

<h4>参考文章</h4>

<p><a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.2.0/CDH4-Installation-Guide/cdh4ig_topic_31.html">http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.2.0/CDH4-Installation-Guide/cdh4ig_topic_31.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[编译cdh47下hive的contrib源码]]></title>
    <link href="http://evoupsight.com/blog/2014/07/31/compile-cdh47-hive-contrib/"/>
    <updated>2014-07-31T10:18:00+08:00</updated>
    <id>http://evoupsight.com/blog/2014/07/31/compile-cdh47-hive-contrib</id>
    <content type="html"><![CDATA[<p>进入hive的src/contrib目录，开始编译，提示需要这几个包得加到classpath中去
<code>
/home/hadoop/software/hadoop-2.0.0-cdh4.7.0/share/hadoop/common/hadoop-common-2.0.0-cdh4.7.0.jar
/home/hadoop/software/hadoop-2.0.0-cdh4.7.0/share/hadoop/mapreduce1/hadoop-core-2.0.0-mr1-cdh4.7.0.jar
</code></p>

<!-- more -->


<p>看下工程路径build.xml的29行
<code>
&lt;import file="../build-common.xml"/&gt;
</code></p>

<p>于是编辑上级目录的build-common.xml文件
<code>
266   &lt;path id="classpath"&gt;
267     &lt;pathelement location="${build.dir.hive}/service/classes"/&gt;
268     &lt;pathelement location="${build.dir.hive}/common/classes"/&gt;
269     &lt;pathelement location="${build.dir.hive}/serde/classes"/&gt;
270     &lt;pathelement location="${build.dir.hive}/metastore/classes"/&gt;
271     &lt;pathelement location="${build.dir.hive}/ql/classes"/&gt;
272     &lt;pathelement location="${build.dir.hive}/beeline/classes"/&gt;
273     &lt;pathelement location="${build.dir.hive}/cli/classes"/&gt;
274     &lt;pathelement location="${build.dir.hive}/shims/classes"/&gt;
275     &lt;pathelement location="${build.dir.hive}/hwi/classes"/&gt;
276     &lt;pathelement location="${build.dir.hive}/jdbc/classes"/&gt;
277     &lt;pathelement location="${build.dir.hive}/hbase-handler/classes"/&gt;
278 &lt;!--{ { {把缺少的hadoop加入--&gt;
279     &lt;pathelement location="/home/hadoop/software/hadoop-2.0.0-cdh4.7.0/share/hadoop/common/hadoop-common-2.0.0-cdh4.7.0.jar"/&gt;
280     &lt;pathelement location="/home/hadoop/software/hadoop-2.0.0-cdh4.7.0/share/hadoop/mapreduce1/hadoop-core-2.0.0-mr1-cdh4.7.0.jar"/&gt;
281 &lt;!--} } }--&gt;
282     &lt;fileset dir="${basedir}" includes="lib/*.jar"/&gt;
283     &lt;path refid="common-classpath"/&gt;
284   &lt;/path&gt;
</code></p>

<p>像上面这样把2个包加入。</p>

<p>然后回到工程文件，执行ant，提示将编译contrib包，一路过去，没有问题
，最后编译的jar包存到了ivy的路径下
```
compile:</p>

<pre><code> [echo] Project: contrib
[javac] Compiling 39 source files to /home/hadoop/software/hive-0.10.0-cdh4.7.0/src/build/contrib/classes
[javac] /home/hadoop/software/hadoop-2.0.0-cdh4.7.0/share/hadoop/common/hadoop-common-2.0.0-cdh4.7.0.jar(org/apache/hadoop/fs/Path.class): 警告: 无法找到类型 'LimitedPrivate' 的注释方法 'value()': 找不到org.apache.hadoop.classification.InterfaceAudience的类文件
[javac] 注: 某些输入文件使用或覆盖了已过时的 API。
[javac] 注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
[javac] 注: /home/hadoop/software/hive-0.10.0-cdh4.7.0/src/contrib/src/java/org/apache/hadoop/hive/contrib/udf/example/UDFExampleStructPrint.java使用了未经检查或不安全的操作。
[javac] 注: 有关详细信息, 请使用 -Xlint:unchecked 重新编译。
[javac] 1 个警告
 [copy] Warning: /home/hadoop/software/hive-0.10.0-cdh4.7.0/src/contrib/src/java/conf does not exist.
</code></pre>

<p>jar:</p>

<pre><code> [echo] Project: contrib
  [jar] Building jar: /home/hadoop/software/hive-0.10.0-cdh4.7.0/src/build/contrib/hive-contrib-0.10.0-cdh4.7.0.jar
</code></pre>

<p>[ivy:publish] :: delivering :: org.apache.hive#hive-contrib;0.10.0-cdh4.7.0 :: 0.10.0-cdh4.7.0 :: integration :: Fri Jul 25 10:54:42 CST 2014
[ivy:publish]   delivering ivy file to /home/hadoop/software/hive-0.10.0-cdh4.7.0/src/build/contrib/ivy-0.10.0-cdh4.7.0.xml
[ivy:publish] :: publishing :: org.apache.hive#hive-contrib
[ivy:publish]   published hive-contrib to /home/hadoop/.ivy2/local/org.apache.hive/hive-contrib/0.10.0-cdh4.7.0/jars/hive-contrib.jar
[ivy:publish]   published ivy to /home/hadoop/.ivy2/local/org.apache.hive/hive-contrib/0.10.0-cdh4.7.0/ivys/ivy.xml</p>

<p>BUILD SUCCESSFUL
Total time: 19 seconds
```
ok
这样就得到了hive-contrib.jar文件,解包看一下内容。没问题，都包含了。</p>

<p>参考互联网文章《如何在Ant中引入第三方Jar包》，链接已挂删除了：）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[hadoop2.0cdh4.6.0完全分布式安装]]></title>
    <link href="http://evoupsight.com/blog/2014/07/10/hadoop2-dot-0cdh4-dot-6-0-fullly-distrbute/"/>
    <updated>2014-07-10T15:57:00+08:00</updated>
    <id>http://evoupsight.com/blog/2014/07/10/hadoop2-dot-0cdh4-dot-6-0-fullly-distrbute</id>
    <content type="html"><![CDATA[<p>hadoop2.0 cdh4安装（完全分布式）</p>

<!-- more -->


<p>vmware版本8.0.4 build-744019</p>

<p>首先规划3台虚拟机
<code>
 ,'''''''''''''''''''''':'''''''''''''''''':''''''''''''''''''''''''''''''''''''''''''''|
 |        usage         |        IP        |                  Hostname                  |
 |                      |                  |                                            |
 |''''''''''''''''''''''|''''''''''''''''''|''''''''''''''''''''''''''''''''''''''''''''|
 | namenode1,datanode1  | 192.168.216.183  |    mdn3namenode1.net,mdn3datanode1.net     |
 |                      |                  |                                            |
 |''''''''''''''''''''''|''''''''''''''''''|''''''''''''''''''''''''''''''''''''''''''''|
 | namenode2,datanode2  | 192.168.216.184  |    mdn3namenode2.net,mdn3datanode2.net     |
 |                      |                  |                                            |
 |''''''''''''''''''''''|''''''''''''''''''|''''''''''''''''''''''''''''''''''''''''''''|
 | datanode2,nfs server | 192.168.216.185  |    mdn3datanode3.net,mdn3nfsserver.net     |
 |                      |                  |                                            |
  '''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
</code></p>

<h3>准备工作</h3>

<p>先安装JDK1.6 linux:先把已经安装的openjdk卸载,安装sun jdk1.6,去oracle下载 （j2se就够了）
<code>bash
$ rpm -qa | grep jdk
java-1.6.0-openjdk-1.6.0.0-1.28.1.10.9.el5_8
$ sudo rpm -e java-1.6.0-openjdk-1.6.0.0-1.28.1.10.9.el5_8
$ sudo chmod +x jdk-6u45-linux-x64-rpm.bin
$ sudo ./jdk-6u45-linux-x64-rpm.bin
</code></p>

<p>hadoop所有操作都是用hadoop帐号，下面添加（如果已经创建了帐号无须添加）
```bash
$ groupadd hadoop
$ useradd -r -g hadoop -d /home/hadoop -m -s /bin/bash hadoop</p>

<p>$ mkdir -p /home/hadoop
$ chgrp -R hadoop /home/hadoop
$ chown -R hadoop /home/hadoop
```</p>

<p>环境变量(在centos里不管编辑~/.profile还是~/.bash_profile都不能加载环境变量，正确的应该是在~/.bashrc中，而如果是root用户，应该可以直接在/etc/profile中编辑)
<code>bash
$ vi ~/.bashrc
export HADOOP_HOME="/usr/local/hadoop"
export JAVA_HOME="/usr/java/jdk1.6.0_45"
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
</code></p>

<p>切换到hadoop帐号，进行免密码的ssh登录设置
<code>bash
$ su hadoop
$ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
$ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys
$ chmod 600 ~/.ssh/authorized_keys
</code></p>

<p>给出我的hadoop/hbase版本
```
Name        : hadoop-hdfs-namenode
Arch        : x86_64
Version     : 2.0.0+1554
Release     : 1.cdh4.6.0.p0.16.el6</p>

<p>Name        : hbase-master
Arch        : x86_64
Version     : 0.94.15+86
Release     : 1.cdh4.6.0.p0.14.el6
```</p>

<p>然后是cdh的软件下载url
<a href="http://archive.cloudera.com/cdh4">http://archive.cloudera.com/cdh4</a>
这个路径下有很多的软件。</p>

<p>下载cdh4.6的几个包安装
<code>bash
$ cd /home/software/
$ wget http://archive.cloudera.com/cdh4/cdh/4/hadoop-2.0.0-cdh4.6.0.tar.gz
$ sudo mkdir /usr/local/hadoop/
$ tar xzf hadoop-2.0.0-cdh4.6.0.tar.gz
$ sudo mv hadoop-2.0.0-cdh4.6.0 /usr/local/
$ sudo mv /usr/local/hadoop-2.0.0-cdh4.6.0 /usr/local/hadoop
$ sudo chown -R hadoop:hadoop /usr/local/hadoop
</code>
创建存储临时文件temp、data和name节点数据的目录
<code>sh
$ sudo mkdir /usr/local/hadoop/temp/ /usr/local/hadoop/data/ /usr/local/hadoop/name/
$ sudo chown -R hadoop:hadoop /usr/local/hadoop
</code></p>

<p>好了，准备工作终了</p>

<p>开始配置
配置/usr/local/hadoop/etc/hadoop/core-site.xml
```xml
<configuration></p>

<pre><code>    &lt;property&gt;
            &lt;name&gt;fs.defaultFS&lt;/name&gt;
            &lt;value&gt;hdfs://mdn3namenode1.net:9000&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;io.file.buffer.size&lt;/name&gt;
            &lt;value&gt;131072&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
            &lt;value&gt;file:/usr/local/hadoop/temp&lt;/value&gt;
            &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;hadoop.proxyuser.hduser.hosts&lt;/name&gt;
            &lt;value&gt;*&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;hadoop.proxyuser.hduser.groups&lt;/name&gt;
            &lt;value&gt;*&lt;/value&gt;
    &lt;/property&gt;
</code></pre>

<p></configuration>
```</p>

<p>配置/usr/local/hadoop/etc/hadoop/hdfs-site.xml
```xml
<configuration></p>

<pre><code>    &lt;property&gt;
            &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
            &lt;value&gt;mdn3namenode1.net:9001&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
    &lt;value&gt;file:/usr/local/hadoop/dfs/name&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
            &lt;value&gt;file:/usr/local/hadoop/dfs/data&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;dfs.replication&lt;/name&gt;
            &lt;value&gt;3&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
            &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
</code></pre>

<p></configuration>
```</p>

<p>配置/usr/local/hadoop/etc/hadoop/madpred-site.xml
<code>sh
cp mapred-site.xml.template mapred-site.xml
</code>
```xml
<configuration></p>

<pre><code>    &lt;property&gt;
            &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
            &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
            &lt;value&gt;mdn3namenode1.net:10020&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
            &lt;value&gt;mdn3namenode1.net:19888&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
</code></pre>

<p></configuration></p>

<p>```</p>

<p>配置/usr/local/hadoop/etc/hadoop/yarn-site.xml
```xml
<configuration></p>

<pre><code>    &lt;property&gt;
            &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
            &lt;value&gt;mapreduce.shuffle&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
            &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
            &lt;value&gt;mdn3namenode1.net:8032&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
            &lt;value&gt;mdn3namenode1.net:8030&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
            &lt;value&gt;mdn3namenode1.net:8031&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
            &lt;value&gt;mdn3namenode1.net:8033&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
            &lt;value&gt;mdn3namenode1.net:8088&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
</code></pre>

<p></configuration>
```</p>

<p>编辑slave的名字
直接讲slave的域名或者slave的ip按照一行一个的规则写进去
<code>
mdn3datanode2.net
mdn3datanode3.net
</code></p>

<p>复制到各台机器上
<code>sh
$ cd /usr/local/
$ sudo scp -dr hadoop@192.168.216.183:/usr/local/hadoop .
$ sudo chown -R hadoop:hadoop hadoop/
</code></p>

<p>格式化hdfs
在namenode上执行
<code>sh
/usr/local/hadoop/bin/hadoop namenode -format
</code></p>

<h3>hbase的安装配置</h3>

<p>hbase依赖zookeeper，需要先去下载
<code>sh
$ cd /home/software/
$ wget http://archive.cloudera.com/cdh4/cdh/4/zookeeper-3.4.5-cdh4.6.0.tar.gz
$ tar xzf zookeeper-3.4.5-cdh4.6.0.tar.gz
$ sudo mv zookeeper-3.4.5-cdh4.6.0 /usr/local/
$ sudo mv /usr/local/zookeeper-3.4.5-cdh4.6.0 /usr/local/zookeeper
$ sudo chown -R hadoop:hadoop /usr/local/zookeeper
$ sudo cp /usr/local/zookeeper/conf/zoo_sample.cfg /usr/local/zookeeper/conf/zoo.cfg
</code>
zookeeper准备完毕，可以继续安装hbase</p>

<p><code>sh
$ cd /home/software/
$ wget http://archive.cloudera.com/cdh4/cdh/4/hbase-0.94.15-cdh4.6.0.tar.gz
$ sudo mkdir /usr/local/hbase/
$ tar xzf hbase-0.94.15-cdh4.6.0.tar.gz
$ sudo mv hbase-0.94.15-cdh4.6.0 /usr/local/
$ sudo mv /usr/local/hbase-0.94.15-cdh4.6.0 /usr/local/hbase
$ sudo chown -R hadoop:hadoop /usr/local/hbase
</code></p>

<p>若干配置步骤
配置hbase-site.xml
```xml
<configuration></p>

<pre><code>&lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;hdfs://mdn3namenode1.net:9000/hbase&lt;/value&gt;

&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hbase.master&lt;/name&gt;
    &lt;value&gt;mdn3datanode1.net:60000&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;mdn3datanode1.net&lt;/value&gt;    &lt;!-- 这里配置若干个zookeeper的服务器地址，需要是奇数个 --&gt;
&lt;/property&gt;
</code></pre>

<p></configuration>
<code>
配置hbase-env.sh
</code>xml
export HBASE_MANAGES_ZK=false
```
不要hbase托管zookeeper</p>

<p>配置regionservers
<code>
mdn3datanode2.net
mdn3datanode3.net
</code></p>

<p>启动hbase
<code>sh
/usr/local/hbase/bin/start-hbase.sh
/usr/local/hbase/bin/hbase-daemons.sh start thrift
</code>
hbase启动完成.</p>

<h3>配置hbase可能碰到几个问题的说明：</h3>

<p>1) 报错
<code>ERROR client.HConnectionManager$HConnectionImplementation: Check the value configured in 'zookeeper.znode.parent'</code></p>

<p>是需要把/etc/hosts中的127.0.0.1注释掉，否则zookeeper还会出现
最后的hosts我这里是这样
```
[hadoop@localhost conf]$ more /etc/hosts</p>

<h1>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</h1>

<p>::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.216.183 mdn3namenode1.net mdn3datanode1.net
192.168.216.184 mdn3namenode2.net mdn3datanode2.net
192.168.216.185 mdn3datanode3.net mdn3nfsserver.net
```</p>

<p>2) 在运行/usr/local/hbase/bin/hbase shell的时候出现了
<code>WARN conf.Configuration: hadoop.native.lib is deprecated. Instead, use io.native.lib.available</code></p>

<p>3) <code>java.net.ConnectException: Connection refused</code>
这是要求hadoop中的slaves配置和hbase的regionservers要一致。</p>

<h3>hive的安装</h3>

<p><code>sh
cd /home/software
wget http://archive.cloudera.com/cdh4/cdh/4/hive-0.10.0-cdh4.6.0.tar.gz
tar xzf hive-0.10.0-cdh4.6.0.tar.gz
sudo mv hive-0.10.0-cdh4.6.0 /usr/local/
sudo mv /usr/local/hive-0.10.0-cdh4.6.0 /usr/local/hive
chown -R hadoop:hadoop /usr/local/hive
</code></p>

<h3>hive的配置</h3>

<p>在~/.bashrc中加入
<code>bash
export HIVE_HOME=/usr/local/hive
export HIVE_CONF_DIR=$HIVE_HOME/conf
export HIVE_LIB=$HIVE_HOME/lib
export PATH=$PATH:$JAVA_HOME/bin:$ZOOKEEPER_HOME:$HIVE_HOME
</code></p>

<p>在conf/hive-site.xml中
```
<configuration>
<property>
  <name>hive.metastore.local</name>
  <value>true</value>
</property></p>

<p><property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://localhost:3306/hive</value>
</property></p>

<p><property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
</property></p>

<p><property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hive</value>
</property></p>

<p><property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>hive</value>
</property>
<property>
  <name>datanucleus.fixedDatastore</name>
  <value>false</value>
</property></p>

<p></configuration></p>

<p>```</p>

<p>这里要安装mysql作为元数据服务器，参考这篇 <a href="http://evoupsight.com/blog/2014/02/17/hadoop0-dot-20-dot-2-plus-hive0-dot-7/">http://evoupsight.com/blog/2014/02/17/hadoop0-dot-20-dot-2-plus-hive0-dot-7/</a></p>

<p>然后/bin/hive后，成功进入shell
```</p>

<blockquote><p>create table test (key string);
<code>``
如果遇到下面的报错
</code> FAILED: Error in metadata: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient `</p></blockquote>

<p><code>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask</code>
建表错误
开始以为hive没有访问mysql的权限,以root用户登录mysql然后赋予hive用户权限：
<code>
grant all privileges on *.* to hive@localhost identified by 'hive';
grant all privileges on *.* to hive@192.168.216.183 identified by 'hive';
</code>
发现问题依旧</p>

<p>其实是要在hive-site.xml中把
<code>
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
  &lt;value&gt;jdbc:mysql://localhost:3306/hive&lt;/value&gt;
&lt;/property&gt;
</code>
改成
<code>
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
  &lt;value&gt;jdbc:mysql://192.168.216.183:3306/hive&lt;/value&gt;
&lt;/property&gt;
</code></p>

<p>问题依旧，打开hive的调试模式
<code>
bin/hive -hiveconf hive.root.logger=DEBUG,console
</code>
<code>14/05/08 17:35:53 WARN conf.HiveConf: DEPRECATED: Configuration property hive.metastore.local no longer has any effect.</code>
<code>Make sure to provide a valid value for hive.metastore.uris if you are connecting to a remote metastore</code></p>

<p>在配置文件里删除hive.metastore.local属性。</p>

<p>最后查得原因是没有安装mysql驱动，只要把mysql-connector-java-5.1.22-bin.jar放到lib下就可以了</p>

<p>然后
```
hive> create table test (key string);
OK
Time taken: 42.259 seconds</p>

<p>hive> show tables;
OK
test
Time taken: 0.279 seconds
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[hive查询报错There are 0 datanode(s) running and no node]]></title>
    <link href="http://evoupsight.com/blog/2014/04/30/hive-there-are-0-datanode-s-running-and-no-node/"/>
    <updated>2014-04-30T17:38:00+08:00</updated>
    <id>http://evoupsight.com/blog/2014/04/30/hive-there-are-0-datanode-s-running-and-no-node</id>
    <content type="html"><![CDATA[<p>hive查询报错<code>c
ould only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation</code></p>

<!-- more -->


<p>```
hive> select count(*) from httplog_2014_05_26_2017;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /tmp/hive-hadoop/hive_2014-07-15_10-27-16_252_580204120341469236-1/-mr-10003/7a56500c-c6e3-4f8c-a32b-053e9fe8eec2 c
ould only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.</p>

<pre><code>    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1340)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2296)
    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:501)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:299)
    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:44954)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1752)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1748)
</code></pre>

<p>```</p>

<p>需要清理目录，其中hadoop为当前用户的名字
<code>bash
cd /tmp/hadoop
rm -rf *
</code></p>

<p>然后重启hadoop和hive，恢复正常，原因是hadoop的namespaceIDs有异常。</p>
]]></content>
  </entry>
  
</feed>
