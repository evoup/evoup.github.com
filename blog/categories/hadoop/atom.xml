<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hadoop | Evoup`s Blog]]></title>
  <link href="http://evoupsight.com/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://evoupsight.com/"/>
  <updated>2014-07-10T16:12:02+08:00</updated>
  <id>http://evoupsight.com/</id>
  <author>
    <name><![CDATA[Evoup`s Blog]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[hadoop2.0cdh4.6.0完全分布式安装]]></title>
    <link href="http://evoupsight.com/blog/2014/07/10/hadoop2-dot-0cdh4-dot-6-0-fullly-distrbute/"/>
    <updated>2014-07-10T15:57:00+08:00</updated>
    <id>http://evoupsight.com/blog/2014/07/10/hadoop2-dot-0cdh4-dot-6-0-fullly-distrbute</id>
    <content type="html"><![CDATA[<p>hadoop2.0 cdh4安装（完全分布式）</p>

<!-- more -->


<p>vmware版本8.0.4 build-744019</p>

<p>首先规划3台虚拟机
<code>
 ,'''''''''''''''''''''':'''''''''''''''''':''''''''''''''''''''''''''''''''''''''''''''|
 |        usage         |        IP        |                  Hostname                  |
 |                      |                  |                                            |
 |''''''''''''''''''''''|''''''''''''''''''|''''''''''''''''''''''''''''''''''''''''''''|
 | namenode1,datanode1  | 192.168.216.183  |    mdn3namenode1.net,mdn3datanode1.net     |
 |                      |                  |                                            |
 |''''''''''''''''''''''|''''''''''''''''''|''''''''''''''''''''''''''''''''''''''''''''|
 | namenode2,datanode2  | 192.168.216.184  |    mdn3namenode2.net,mdn3datanode2.net     |
 |                      |                  |                                            |
 |''''''''''''''''''''''|''''''''''''''''''|''''''''''''''''''''''''''''''''''''''''''''|
 | datanode2,nfs server | 192.168.216.185  |    mdn3datanode3.net,mdn3nfsserver.net     |
 |                      |                  |                                            |
  '''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
</code></p>

<h3>准备工作</h3>

<p>先安装JDK1.6 linux:先把已经安装的openjdk卸载,安装sun jdk1.6,去oracle下载 （j2se就够了）
<code>bash
$ rpm -qa | grep jdk
java-1.6.0-openjdk-1.6.0.0-1.28.1.10.9.el5_8
$ sudo rpm -e java-1.6.0-openjdk-1.6.0.0-1.28.1.10.9.el5_8
$ sudo chmod +x jdk-6u45-linux-x64-rpm.bin
$ sudo ./jdk-6u45-linux-x64-rpm.bin
</code></p>

<p>hadoop所有操作都是用hadoop帐号，下面添加
```bash
$ groupadd hadoop
$ useradd -r -g hadoop -d /home/hadoop -m -s /bin/bash hadoop</p>

<p>$ mkdir -p /u01/app
$ chgrp -R hadoop /u01/app
$ chown -R hadoop /u01/app
```</p>

<p>环境变量(在centos里不管编辑~/.profile还是~/.bash_profile都不能加载环境变量，正确的应该是在~/.bashrc中，而如果是root用户，应该可以直接在/etc/profile中编辑)
<code>bash
$ vi ~/.bashrc
export HADOOP_HOME="/usr/local/hadoop"
export JAVA_HOME="/usr/java/jdk1.6.0_45"
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
</code></p>

<p>切换到hadoop帐号，进行免密码的ssh登录设置
<code>bash
$ su hadoop
$ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
$ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys
$ chmod 600 ~/.ssh/authorized_keys
</code></p>

<p>给出我的hadoop/hbase版本
```
Name        : hadoop-hdfs-namenode
Arch        : x86_64
Version     : 2.0.0+1554
Release     : 1.cdh4.6.0.p0.16.el6</p>

<p>Name        : hbase-master
Arch        : x86_64
Version     : 0.94.15+86
Release     : 1.cdh4.6.0.p0.14.el6
```</p>

<p>然后是cdh的软件下载url
<a href="http://archive.cloudera.com/cdh4">http://archive.cloudera.com/cdh4</a>
这个路径下有很多的软件。</p>

<p>下载cdh4.6的几个包安装
<code>bash
$ cd /home/software/
$ wget http://archive.cloudera.com/cdh4/cdh/4/hadoop-2.0.0-cdh4.6.0.tar.gz
$ sudo mkdir /usr/local/hadoop/
$ tar xzf hadoop-2.0.0-cdh4.6.0.tar.gz
$ sudo mv hadoop-2.0.0-cdh4.6.0 /usr/local/
$ sudo mv /usr/local/hadoop-2.0.0-cdh4.6.0 /usr/local/hadoop
$ sudo chown -R hadoop:hadoop /usr/local/hadoop
</code>
创建存储临时文件temp、data和name节点数据的目录
<code>sh
$ sudo mkdir /usr/local/hadoop/temp/ /usr/local/hadoop/data/ /usr/local/hadoop/name/
</code></p>

<p>好了，准备工作终了</p>

<p>开始配置
配置/usr/local/hadoop/etc/hadoop/core-site.xml
```xml
<configuration></p>

<pre><code>    &lt;property&gt;
            &lt;name&gt;fs.defaultFS&lt;/name&gt;
            &lt;value&gt;hdfs://mdn3namenode1.net:9000&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;io.file.buffer.size&lt;/name&gt;
            &lt;value&gt;131072&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
            &lt;value&gt;file:/usr/local/hadoop/temp&lt;/value&gt;
            &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;hadoop.proxyuser.hduser.hosts&lt;/name&gt;
            &lt;value&gt;*&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;hadoop.proxyuser.hduser.groups&lt;/name&gt;
            &lt;value&gt;*&lt;/value&gt;
    &lt;/property&gt;
</code></pre>

<p></configuration>
```</p>

<p>配置/usr/local/hadoop/etc/hadoop/hdfs-site.xml
```xml
<configuration></p>

<pre><code>    &lt;property&gt;
            &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
            &lt;value&gt;mdn3namenode1.net:9001&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
    &lt;value&gt;file:/usr/local/hadoop/dfs/name&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
            &lt;value&gt;file:/usr/local/hadoop/dfs/data&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;dfs.replication&lt;/name&gt;
            &lt;value&gt;3&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
            &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
</code></pre>

<p></configuration>
```</p>

<p>配置/usr/local/hadoop/etc/hadoop/madpred-site.xml
<code>sh
cp mapred-site.xml.template mapred-site.xml
</code>
```xml
<configuration></p>

<pre><code>    &lt;property&gt;
            &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
            &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
            &lt;value&gt;mdn3namenode1.net:10020&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
            &lt;value&gt;mdn3namenode1.net:19888&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
</code></pre>

<p></configuration></p>

<p>```</p>

<p>配置/usr/local/hadoop/etc/hadoop/yarn-site.xml
```xml
<configuration></p>

<pre><code>    &lt;property&gt;
            &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
            &lt;value&gt;mapreduce.shuffle&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
            &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
            &lt;value&gt;mdn3namenode1.net:8032&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
            &lt;value&gt;mdn3namenode1.net:8030&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
            &lt;value&gt;mdn3namenode1.net:8031&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
            &lt;value&gt;mdn3namenode1.net:8033&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
            &lt;value&gt;mdn3namenode1.net:8088&lt;/value&gt; &lt;!-- master域名或者master的ip --&gt;
    &lt;/property&gt;
</code></pre>

<p></configuration>
```</p>

<p>编辑slave的名字
直接讲slave的域名或者slave的ip按照一行一个的规则写进去
<code>
mdn3datanode2.net
mdn3datanode3.net
</code></p>

<p>复制到各台机器上
<code>sh
$ cd /usr/local/
$ sudo scp -dr hadoop@192.168.216.183:/usr/local/hadoop .
$ sudo chown -R hadoop:hadoop hadoop/
</code></p>

<p>格式化hdfs
在namenode上执行
<code>sh
/usr/local/hadoop/bin/hadoop namenode -format
</code></p>

<h3>hbase的安装配置</h3>

<p>hbase依赖zookeeper，需要先去下载
<code>sh
$ cd /home/software/
$ wget http://archive.cloudera.com/cdh4/cdh/4/zookeeper-3.4.5-cdh4.6.0.tar.gz
$ tar xzf zookeeper-3.4.5-cdh4.6.0.tar.gz
$ sudo mv zookeeper-3.4.5-cdh4.6.0 /usr/local/
$ sudo mv /usr/local/zookeeper-3.4.5-cdh4.6.0 /usr/local/zookeeper
$ sudo chown -R hadoop:hadoop /usr/local/zookeeper
$ sudo cp /usr/local/zookeeper/conf/zoo_sample.cfg /usr/local/zookeeper/conf/zoo.cfg
</code>
zookeeper准备完毕，可以继续安装hbase</p>

<p><code>sh
$ cd /home/software/
$ wget http://archive.cloudera.com/cdh4/cdh/4/hbase-0.94.15-cdh4.6.0.tar.gz
$ sudo mkdir /usr/local/hbase/
$ tar xzf hbase-0.94.15-cdh4.6.0.tar.gz
$ sudo mv hbase-0.94.15-cdh4.6.0 /usr/local/
$ sudo mv /usr/local/hbase-0.94.15-cdh4.6.0 /usr/local/hbase
$ sudo chown -R hadoop:hadoop /usr/local/hbase
</code></p>

<p>若干配置步骤
配置hbase-site.xml
```xml
<configuration></p>

<pre><code>&lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;hdfs://mdn3namenode1.net:9000/hbase&lt;/value&gt;

&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hbase.master&lt;/name&gt;
    &lt;value&gt;mdn3datanode1.net:60000&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;mdn3datanode1.net&lt;/value&gt;    &lt;!-- 这里配置若干个zookeeper的服务器地址，需要是奇数个 --&gt;
&lt;/property&gt;
</code></pre>

<p></configuration>
<code>
配置hbase-env.sh
</code>xml
export HBASE_MANAGES_ZK=false
```
不要hbase托管zookeeper</p>

<p>配置regionservers
<code>
mdn3datanode2.net
mdn3datanode3.net
</code></p>

<p>启动hbase
<code>sh
/usr/local/hbase/bin/start-hbase.sh
/usr/local/hbase/bin/hbase-daemons.sh start thrift
</code>
hbase启动完成.</p>

<h3>配置hbase可能碰到几个问题的说明：</h3>

<p>1) 报错
<code>ERROR client.HConnectionManager$HConnectionImplementation: Check the value configured in 'zookeeper.znode.parent'</code></p>

<p>是需要把/etc/hosts中的127.0.0.1注释掉，否则zookeeper还会出现</p>

<p>2) 在运行/usr/local/hbase/bin/hbase shell的时候出现了
<code>WARN conf.Configuration: hadoop.native.lib is deprecated. Instead, use io.native.lib.available</code></p>

<p>3) <code>java.net.ConnectException: Connection refused</code>
这是要求hadoop中的slaves配置和hbase的regionservers要一致。</p>

<h3>hive的安装</h3>

<p><code>sh
cd /home/software
wget http://archive.cloudera.com/cdh4/cdh/4/hive-0.10.0-cdh4.6.0.tar.gz
tar xzf hive-0.10.0-cdh4.6.0.tar.gz
sudo mv hive-0.10.0-cdh4.6.0 /usr/local/
sudo mv /usr/local/hive-0.10.0-cdh4.6.0 /usr/local/hive
chown -R hadoop:hadoop /usr/local/hive
</code></p>

<h3>hive的配置</h3>

<p>在~/.bashrc中加入
<code>bash
export HIVE_HOME=/usr/local/hive
export HIVE_CONF_DIR=$HIVE_HOME/conf
export HIVE_LIB=$HIVE_HOME/lib
export PATH=$PATH:$JAVA_HOME/bin:$ZOOKEEPER_HOME:$HIVE_HOME
</code></p>

<p>在conf/hive-site.xml中
```
 version=&ldquo;1.0&rdquo;?>
&lt;?xml-stylesheet type=&ldquo;text/xsl&rdquo; href=&ldquo;configuration.xsl&rdquo;?></p>

<p><configuration>
<property>
  <name>hive.metastore.local</name>
  <value>true</value>
</property></p>

<p><property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://localhost:3306/hive</value>
</property></p>

<p><property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
</property></p>

<p><property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hive</value>
</property></p>

<p><property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>hive</value>
</property>
<property>
  <name>datanucleus.fixedDatastore</name>
  <value>false</value>
</property></p>

<p></configuration></p>

<p>```</p>

<p>然后/bin/hive后，成功进入shell
```</p>

<blockquote><p>create table test (key string);
<code>``
如果遇到下面的报错
</code> FAILED: Error in metadata: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient `</p></blockquote>

<p><code>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask</code>
建表错误
开始以为hive没有访问mysql的权限,以root用户登录mysql然后赋予hive用户权限：
<code>
grant all privileges on *.* to hive@localhost identified by 'hive';
grant all privileges on *.* to hive@192.168.216.183 identified by 'hive';
</code>
发现问题依旧</p>

<p>其实是要在hive-site.xml中把
<code>
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
  &lt;value&gt;jdbc:mysql://localhost:3306/hive&lt;/value&gt;
&lt;/property&gt;
</code>
改成
<code>
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
  &lt;value&gt;jdbc:mysql://192.168.216.183:3306/hive&lt;/value&gt;
&lt;/property&gt;
</code></p>

<p>问题依旧，打开hive的调试模式
<code>
bin/hive -hiveconf hive.root.logger=DEBUG,console
</code>
<code>14/05/08 17:35:53 WARN conf.HiveConf: DEPRECATED: Configuration property hive.metastore.local no longer has any effect.</code>
<code>Make sure to provide a valid value for hive.metastore.uris if you are connecting to a remote metastore</code></p>

<p>在配置文件里删除hive.metastore.local属性。</p>

<p>最后查得原因是没有安装mysql驱动，只要把mysql-connector-java-5.1.22-bin.jar放到lib下就可以了</p>

<p>然后
```
hive> create table test (key string);
OK
Time taken: 42.259 seconds</p>

<p>hive> show tables;
OK
test
Time taken: 0.279 seconds
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[导入nginx日志并采用hive进行统计]]></title>
    <link href="http://evoupsight.com/blog/2014/03/10/import-nginx-log-and-use-hive-to-caculate/"/>
    <updated>2014-03-10T16:07:00+08:00</updated>
    <id>http://evoupsight.com/blog/2014/03/10/import-nginx-log-and-use-hive-to-caculate</id>
    <content type="html"><![CDATA[<p>公司的日志本来是存成gzip或bz2格式直接导入到hdfs里去然后用程序直接mr的，速度是比较慢的。领导要求采用hive来计算，而在hive里建textfile表的这种方式，textfile是无法进行并行计算的，而且gzip和bz2做mr的速度是很慢的。所以准备采取先导入hdfs和textfile表，然后再转换为rcfile格式的表的策略。实际试验下来，如果一开始转换为文本或者是lzo格式，而不是采用gzip或bz2的格式的textfile的表，再转换为rcfile的方式会快很多，mr的速度也是比较快的。</p>

<p>下面描述一下过程</p>

<!-- more -->


<p>把全部日志上通过scp等方式传到服务器之后，要做的是先建一个textfile的表
<code>sh
create external table nginxlog (ipaddress string, ...更多字段省略) COMMENT 'nginx log' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile    location '/data/log';
</code></p>

<p>把得到所有nginx日志，然后用lzop先压缩好，然后传到hdfs上去。
<code>sh
$ tar xjf server01-20140131.txt.bz2
$ tar xjf server02-20140131.txt.bz2
$ lzop server01-20140131.txt
$ lzop server02-20140131.txt
$ server01-20140131.txt.lzo server02-20140131.txt.lzo
$ /u01/app/hadoop fs -put server01-20140131.txt.lzo /data/log/server01-20140131.txt.lzo
$ /u01/app/hadoop fs -put server01-20140131.txt.lzo /data/log/server02-20140131.txt.lzo
$/u01/app/hadoop/bin/hadoop fs -ls /data/log/
Found 2 items
-rw-r--r--   1 hadoop supergroup  364459530 2014-03-07 18:10 /data/log/server01-20140131.txt.lzo
-rw-r--r--   1 hadoop supergroup  364459530 2014-03-10 13:31 /data/log/server02-20140201.txt.lzo
</code></p>

<p>然后马上就可以查询了
<code>
hive&gt; select count(*) from nginxlog;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=&lt;number&gt;
In order to set a constant number of reducers:
  set mapred.reduce.tasks=&lt;number&gt;
Starting Job = job_201403101051_0004, Tracking URL = http://mdn2.net:50030/jobdetails.jsp?jobid=job_201403101051_0004
Kill Command = /u01/app/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=mdn2.net:9025 -kill job_201403101051_0004
2014-03-10 14:17:41,179 Stage-1 map = 0%,  reduce = 0%
2014-03-10 14:18:20,815 Stage-1 map = 50%,  reduce = 0%
2014-03-10 14:18:32,927 Stage-1 map = 100%,  reduce = 0%
2014-03-10 14:18:38,971 Stage-1 map = 100%,  reduce = 17%
2014-03-10 14:18:41,991 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201403101051_0004
OK
2492916
Time taken: 75.321 seconds
</code></p>

<p>这么做也是可以使用hive的，但是速度还是比较慢。于是可以再创建一个rcfile格式的表，然后再查询
<code>
bin/hive&gt; create external table nginxlog2 (ipaddress string, ...,更多字段) COMMENT 'nginx log rcfile format' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as RCFile    location '/data/log2';
</code></p>

<p>然后用
<code>
hive&gt; SET hive.exec.compress.output=true;
hive&gt; SET io.rcfile.compression.type=BLOCK;
hive&gt; insert overwrite table nginxlog2 select * from nginxlog;
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201403101051_0007, Tracking URL = http://mdn2.net:50030/jobdetails.jsp?jobid=job_201403101051_0007
Kill Command = /u01/app/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=mdn2.net:9025 -kill job_201403101051_0007
2014-03-10 15:20:20,959 Stage-1 map = 0%,  reduce = 0%
2014-03-10 15:21:21,267 Stage-1 map = 0%,  reduce = 0%
2014-03-10 15:22:21,627 Stage-1 map = 0%,  reduce = 0%
2014-03-10 15:23:22,320 Stage-1 map = 0%,  reduce = 0%
2014-03-10 15:23:36,542 Stage-1 map = 100%,  reduce = 0%
2014-03-10 15:23:42,665 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201403101051_0007
Ended Job = -1308159129, job is filtered out (removed at runtime).
Moving data to: hdfs://mdn2.net:9024/tmp/hive-hadoop/hive_2014-03-10_15-20-15_076_2561493179927538497/-ext-10000
Loading data to table default.nginxlog2
Deleted hdfs://mdn2.net:9024/data/log2
Table default.nginxlog2 stats: [num_partitions: 0, num_files: 0, num_rows: 0, total_size: 0]
2492916 Rows loaded to nginxlog2
OK
Time taken: 209.088 seconds
</code></p>

<p>然后再次select，对比一下时间
<code>
hive&gt; select count(*) from nginxlog2;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=&lt;number&gt;
In order to set a constant number of reducers:
  set mapred.reduce.tasks=&lt;number&gt;
Starting Job = job_201403101051_0008, Tracking URL = http://mdn2.net:50030/jobdetails.jsp?jobid=job_201403101051_0008
Kill Command = /u01/app/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=mdn2.net:9025 -kill job_201403101051_0008
2014-03-10 15:26:21,984 Stage-1 map = 0%,  reduce = 0%
2014-03-10 15:26:31,031 Stage-1 map = 33%,  reduce = 0%
2014-03-10 15:26:43,107 Stage-1 map = 67%,  reduce = 0%
2014-03-10 15:26:49,140 Stage-1 map = 67%,  reduce = 17%
2014-03-10 15:26:52,153 Stage-1 map = 100%,  reduce = 22%
2014-03-10 15:27:04,225 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201403101051_0008
OK
2492916
Time taken: 55.656 seconds
</code>
我这里是2个节点， 55.656s，相比textfile的75.321s，rcfile的有20秒的优势，当然并行计算的节点越多，时间就越省。</p>

<p>这里要补充一下：不通过本地导入的方式直接导入rcfile的原因，是因为textfile格式才支持从本地导入，sequencefile和rcfile均不支持，所以只能先搞一个表再复制。如果用textfile加gzip或bz2的表再复制到rcfile的表，时间会很长；而用textfile+lzo的表再复制到rcfile的表，时间比较短。lzo相对gzip或bz2压缩速度快但是相对压缩比没有优势，然而再转为rcfile格式mr会很快，这样hive查询就很快。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[hadoop报错：Incompatible namespaceIDs]]></title>
    <link href="http://evoupsight.com/blog/2014/02/24/hadoop-error-incompatible-namespaceids/"/>
    <updated>2014-02-24T17:08:00+08:00</updated>
    <id>http://evoupsight.com/blog/2014/02/24/hadoop-error-incompatible-namespaceids</id>
    <content type="html"><![CDATA[<p>今天在修改hadoop主机名重新格式化namenode之后，重新启动hadoop，发现datanode无法启动起来。</p>

<p>报错：HADOOP报错<code>Incompatible namespaceIDs</code></p>

<p>查看报告发现没有启动一个datanode</p>

<!-- more -->


<p>```sh
$/u01/app/hadoop/bin/hadoop dfsadmin -report
Configured Capacity: 0 (0 KB)
Present Capacity: 0 (0 KB)
DFS Remaining: 0 (0 KB)
DFS Used: 0 (0 KB)
DFS Used%: �%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0</p>

<hr />

<p>Datanodes available: 0 (0 total, 0 dead)
```</p>

<p>原来是要求datanode的VERSION文件和namenode的要一致</p>

<p>于是到namenode上看文件
```sh
[hadoop@mdn2 current]$more /u01/app/hadoopTmp/dfs/name/current/VERSION</p>

<h1>Mon Feb 24 16:48:12 CST 2014</h1>

<p>namespaceID=1235115105
cTime=0
storageType=NAME_NODE
layoutVersion=-31
```
namespaceID为1235115105</p>

<p>到datanode里查看发现不存在</p>

<h3>解决方法两种任选其一：</h3>

<p>1）在datanode的&lt;dfs.data.dir>/current/VERSION中指定一个一模一样的namespaceID=1235115105，然后重启datanode</p>

<p>2）在格式化namenode的时候要清空/tmp目录下所有有关hadoop的目录，不论是namenode还是datanode所在的机器</p>

<h4>参考文章</h4>

<p><a href="http://blog.csdn.net/wanghai__/article/details/5752199">http://blog.csdn.net/wanghai__/article/details/5752199</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[hive初试--导入数据和查询]]></title>
    <link href="http://evoupsight.com/blog/2014/02/20/hive-import-data/"/>
    <updated>2014-02-20T10:50:00+08:00</updated>
    <id>http://evoupsight.com/blog/2014/02/20/hive-import-data</id>
    <content type="html"><![CDATA[<p>hive虽然是基于hadoop的map/reduce进行云计算，但是自身需要依赖一个元数据表，要么是derby，要么是mysql，相同点总归是要先导入数据，然后才能进行处理。其原理是把结构化的数据文件映射为一张数据库表，然后将SQL语句转化为MapReduce任务进行运行，以绕过专门开发MapReduce这样一个逆向思维的产物。</p>

<p>Hive不可以改写、插入和删除数据，换句话说hive完全就是用来进行计算的。</p>

<p>Hive的数据是存在hdfs上的,所以数据导入之后除了元数据之外，还有另一份本体数据（通常比较大的）存在hdfs上。</p>

<p>有了基础概念之后，开始正题了。</p>

<!-- more -->


<h3>环境描述</h3>

<p>hadoop0.20.203+hive0.7</p>

<h3>任务描述</h3>

<p>本次目的，是把一张二维表导入到hive中后，然后根据编号查询对应的单词。</p>

<h3>过程描述</h3>

<p>假设有这样一个文件test.txt</p>

<p><img src="/images/evoup/hive_test_txt.png" alt="Alt text" /></p>

<p>(vim党注意：如果你已经把tab键映射为4个空格，那么请进入插入模式后在数字后ctrl+v,然后按下<tab>键，再输入单词，否则无法完成制表符的键入，数据导入失败。)</p>

<p>启动hive建表:
```sh
hive>  CREATE EXTERNAL TABLE MYTEST(id INT, name STRING)</p>

<blockquote><p>COMMENT &lsquo;this is a test&rsquo;
ROW FORMAT DELIMITED FIELDS TERMINATED BY &lsquo;\t&rsquo;
STORED AS TEXTFILE
LOCATION &lsquo;/data/test&rsquo;;
OK
```
注意这一步要求原本的hdfs目录下没有/data/test文件夹，如果有的话，hive是要报错的。
还有存储格式有三种textfile、rcfile和sequencefile。其中多数情况用textfile就可以了，如果要压缩，可以考虑后两者。</p></blockquote>

<p>进入hadoop，开始导入
<code>sh
/bin/hadoop fs -put test.txt /data/test
</code></p>

<p>回到hive，用简单的HQL查询语句查询id为4的记录
<code>sh
hive&gt; select * from mytest where id = 4;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201402191826_0007, Tracking URL = http://mdn2.net:50030/jobdetails.jsp?jobid=job_201402191826_0007
Kill Command = /u01/app/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=mdn2.net:9025 -kill job_201402191826_0007
2014-02-20 00:16:34,842 Stage-1 map = 0%,  reduce = 0%
2014-02-20 00:16:40,889 Stage-1 map = 100%,  reduce = 0%
2014-02-20 00:16:46,936 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191826_0007
OK
4       case
Time taken: 21.36 seconds
</code></p>

<p>hive查询一次需要21秒?没错，这就是MapReduce查询的特点了，换做mysql的话这样查询一次应该是&lt;1秒的。好啦，收工。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[centos5.8下基于hadoop0.20.203的hive0.7安装]]></title>
    <link href="http://evoupsight.com/blog/2014/02/17/hadoop0-dot-20-dot-2-plus-hive0-dot-7/"/>
    <updated>2014-02-17T15:38:00+08:00</updated>
    <id>http://evoupsight.com/blog/2014/02/17/hadoop0-dot-20-dot-2-plus-hive0-dot-7</id>
    <content type="html"><![CDATA[<h3>什么是hive</h3>

<blockquote><p>hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p></blockquote>

<h3>hive使用场景</h3>

<blockquote><p>HIVE 不适合用于联机(online)事务处理,也不提供实时查询功能。它最适合应用在基于大量不可变数据的批处理作业。</p></blockquote>

<!-- more -->


<h3>hive的安装</h3>

<p>先提一下，之前我的hadoop安装在/u01/app/hadoop目录下，同样的我们下载到改目录下，然后开始安装
注意我的hadoop版本为hadooop-0.20.203.0.tgz，与此匹配的版本为hive0.7.0
<code>sh
cd /u01/app
wget http://archive.apache.org/dist/hive/hive-0.7.1/hive-0.7.1-bin.tar.gz
tar xzf hive-0.7.1-bin.tar.gz
ln -s hive-0.7.1-bin.tar.gz hive
</code></p>

<h4>环境变量设置</h4>

<p>在~/.profile中加入
<code>sh
export HIVE_HOME=/u01/app/hive
export HIVE_CONF_DIR=/u01/app/hive/conf
</code></p>

<p>在系统中指出hive的配置文件所在
<code>sh
export PATH=$HIVE_HOME/bin:PATH
</code>
这个实现输入hive，hive service就会自动相应，而不用输入hive所在的绝对路径。
<code>sh
export HIVE_LIB=$HIVE_HOME/lib
</code></p>

<p>记得用source让profile生效
<code>sh
source ~/.profile
</code></p>

<p>然后是进行hive配置文件的配置
<code>sh
cd /u01/app/hive/conf
cp hive-env.sh.template hive-env.sh
vim hive-env.sh
</code></p>

<h4>安装依赖软件</h4>

<p>发现有2种安装方式，一种是derby,另一种是mysql，这里先介绍mysql方式</p>

<blockquote><p>关于什么是derby: 这是一个apache DB的子项目，是一个完全用java实现的开源关系型数据库。这里就不使用了，我们采用mysql。</p></blockquote>

<h4>安装mysql</h4>

<p><code>sh
//卸载老版本的mysql软件包
yum remove mysql mysql-*
//安装mysql5.5的源
rpm -Uvh http://repo.webtatic.com/yum/centos/5/latest.rpm
//安装MySQL客户端的支持包
yum install libmysqlclient15 --enablerepo=webtatic
//安装MySQL 5.5的客户端和服务端
yum install mysql55 mysql55-server --enablerepo=webtatic
//启动MySQL系统服务，更新数据库
/etc/init.d/mysqld restart
mysql_upgrade
</code></p>

<h4>修改mysql用户密码</h4>

<p>```sh</p>

<h1>mysql -u root mysql   //默认的没有密码直接进去的</h1>

<p>mysql>use mysql;
mysql>desc user;
mysql> GRANT ALL PRIVILEGES ON <em>.</em> TO root@&ldquo;%&rdquo; IDENTIFIED BY &ldquo;root&rdquo;;　　//为root添加远程连接的能力。
mysql>update user set Password = password(&lsquo;xxxxxx&rsquo;) where User=&lsquo;root&rsquo;;
mysql>select Host,User,Password  from user where User=&lsquo;root&rsquo;;
mysql>flush privileges;
mysql>exit
```</p>

<h4>设置mysql为开机自动启动</h4>

<p><code>sh
sudo /sbin/chkconfig --add mysqld
sudo /sbin/chkconfig mysqld on
</code></p>

<h4>开始配置</h4>

<p>在conf目录下创建hive-site.xml</p>

<p>创建hive数据库给hive做元数据表
<code>sh
create database hive;
grant all privileges on *.* to hive@localhost identified by 'hive';
flush privileges;
</code></p>

<p>运行hive
```sh
cd /u01/app/hive
/bin/hive
Exception in thread &ldquo;main&rdquo; java.lang.NoClassDefFoundError: jline/ArgumentCompletor$ArgumentDelimiter</p>

<pre><code>    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:247)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:149)
</code></pre>

<p>Caused by: java.lang.ClassNotFoundException: jline.ArgumentCompletor$ArgumentDelimiter</p>

<pre><code>    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
    ... 3 more
</code></pre>

<p>```
这个只需要把hive/lib下的jline-0.9.94.jar复制到$HADOOP/lib下即可。</p>

<p>再次启动
```sh
bin/hive
Exception in thread &ldquo;main&rdquo; java.lang.NoClassDefFoundError: org/apache/hadoop/hive/conf/HiveConf</p>

<pre><code>    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:247)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:149)
</code></pre>

<p>Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf</p>

<pre><code>    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
</code></pre>

<p>```</p>

<p>需要修改$HADOOP/conf/的hadoop-env.sh中的
<code>sh
export HADOOP_CLASSPATH=$HBASE_HOME/hbase-0.90.3.jar:$HBASE_HOME:$HBASE_HOME/lib/zookeeper-3.2.2.jar:$HBASE_HOME/conf
</code></p>

<p>改成</p>

<p><code>sh
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HBASE_HOME/hbase-0.90.3.jar:$HBASE_HOME:$HBASE_HOME/lib/zookeeper-3.2.2.jar:$HBASE_HOME/conf
</code></p>

<p>然后可以启动hive了
<code>sh
bin/hive
WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201402170220_1889385824.txt
hive&gt;
</code>
有警告，估计是jdk我用的1.7导致的，可以先不管，接下来可以试试hive的操作了。</p>

<p>•建立测试表test
```sh</p>

<blockquote><p>create table test (key string);
show tables;
FAILED: Error in metadata: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
NestedThrowables:
java.lang.reflect.InvocationTargetException
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
```</p></blockquote>

<p>原来如果mysql用rpm安装，还需要一个jar包mysql-connector-java-5.15-bin.jar，然后拷贝到hive的lib目录下可以。</p>

<p>``sh</p>

<blockquote><p>show tables;
OK
Time taken: 0.082 seconds
```</p></blockquote>

<p>一个表也没有，创建表吧
···sh</p>

<blockquote><p>create table test (key string);
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /user/hive/warehouse/test. Name node is in safe mode.
```</p></blockquote>

<p>namenode为什么是安全模式？
hadoop启动的时候是在安全模式，查看一下现在的模式状态
<code>sh
bin/hadoop dfsadmin –safemode get
ON
</code></p>

<p>那就关了
<code>sh
bin/hadoop dfsadmin -safemode leave
</code></p>

<p>再次查看
<code>sh
bin/hadoop dfsadmin –safemode get
OFF
</code></p>

<p>已经关了，再次建表测试
<code>sh
bin/hive
hive&gt; create table test (key string);
OK
Time taken: 0.521 seconds
hive&gt; show tables;
OK
test
Time taken: 0.14 seconds
</code></p>

<h3>参考文章</h3>

<p><a href="http://blog.163.com/huang_zhong_yuan/blog/static/174975283201181371146365/">http://blog.163.com/huang_zhong_yuan/blog/static/174975283201181371146365/</a></p>

<p><a href="http://hi.baidu.com/allense7en/item/db8e5b4fb177aae81e19bcb4">http://hi.baidu.com/allense7en/item/db8e5b4fb177aae81e19bcb4</a></p>

<p><a href="http://www.cnblogs.com/zhanghuijunjava/archive/2013/04/22/hadoop_HDFS.html">http://www.cnblogs.com/zhanghuijunjava/archive/2013/04/22/hadoop_HDFS.html</a></p>
]]></content>
  </entry>
  
</feed>
