<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hbase | Evoup`s Blog]]></title>
  <link href="http://evoupsight.com/blog/categories/hbase/atom.xml" rel="self"/>
  <link href="http://evoupsight.com/"/>
  <updated>2016-08-03T12:54:25+08:00</updated>
  <id>http://evoupsight.com/</id>
  <author>
    <name><![CDATA[Evoup`s Blog]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[hadoop2.0cdh4.6.0完全分布式安装]]></title>
    <link href="http://evoupsight.com/blog/2014/07/10/hadoop2-dot-0cdh4-dot-6-0-fullly-distrbute/"/>
    <updated>2014-07-10T15:57:00+08:00</updated>
    <id>http://evoupsight.com/blog/2014/07/10/hadoop2-dot-0cdh4-dot-6-0-fullly-distrbute</id>
    <content type="html"><![CDATA[<p>hadoop2.0 cdh4安装（完全分布式）</p>

<!-- more -->

<p>vmware版本8.0.4 build-744019</p>

<p>首先规划3台虚拟机</p>

<p><code>bash
 ,'''''''''''''''''''''':'''''''''''''''''':''''''''''''''''''''''''''''''''''''''''''''|
 |        usage         |        IP        |                  Hostname                  |
 |                      |                  |                                            |
 |''''''''''''''''''''''|''''''''''''''''''|''''''''''''''''''''''''''''''''''''''''''''|
 | namenode1,datanode1  | 192.168.216.183  |    mdn3namenode1.net,mdn3datanode1.net     |
 |                      |                  |                                            |
 |''''''''''''''''''''''|''''''''''''''''''|''''''''''''''''''''''''''''''''''''''''''''|
 | namenode2,datanode2  | 192.168.216.184  |    mdn3namenode2.net,mdn3datanode2.net     |
 |                      |                  |                                            |
 |''''''''''''''''''''''|''''''''''''''''''|''''''''''''''''''''''''''''''''''''''''''''|
 | datanode2,nfs server | 192.168.216.185  |    mdn3datanode3.net,mdn3nfsserver.net     |
 |                      |                  |                                            |
  '''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
</code></p>

<h3 id="section">准备工作</h3>

<p>先安装JDK1.6 linux:先把已经安装的openjdk卸载,安装sun jdk1.6,去oracle下载 （j2se就够了）</p>

<p><code>bash
$ rpm -qa | grep jdk
java-1.6.0-openjdk-1.6.0.0-1.28.1.10.9.el5_8
$ sudo rpm -e java-1.6.0-openjdk-1.6.0.0-1.28.1.10.9.el5_8
$ sudo chmod +x jdk-6u45-linux-x64-rpm.bin
$ sudo ./jdk-6u45-linux-x64-rpm.bin
</code></p>

<p>hadoop所有操作都是用hadoop帐号，下面添加（如果已经创建了帐号无须添加）</p>

<p>```bash
$ groupadd hadoop
$ useradd -r -g hadoop -d /home/hadoop -m -s /bin/bash hadoop</p>

<p>$ mkdir -p /home/hadoop
$ chgrp -R hadoop /home/hadoop
$ chown -R hadoop /home/hadoop
```</p>

<p>环境变量(在centos里不管编辑~/.profile还是~/.bash_profile都不能加载环境变量，正确的应该是在~/.bashrc中，而如果是root用户，应该可以直接在/etc/profile中编辑)</p>

<p><code>bash
$ vi ~/.bashrc 
export HADOOP_HOME="/usr/local/hadoop"
export JAVA_HOME="/usr/java/jdk1.6.0_45"
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
</code></p>

<p>切换到hadoop帐号，进行免密码的ssh登录设置</p>

<p><code>bash
$ su hadoop
$ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
$ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys
$ chmod 600 ~/.ssh/authorized_keys
</code></p>

<p>给出我的hadoop/hbase版本</p>

<p>```bash
Name        : hadoop-hdfs-namenode
Arch        : x86_64
Version     : 2.0.0+1554
Release     : 1.cdh4.6.0.p0.16.el6</p>

<p>Name        : hbase-master
Arch        : x86_64
Version     : 0.94.15+86
Release     : 1.cdh4.6.0.p0.14.el6
```</p>

<p>然后是cdh的软件下载url
http://archive.cloudera.com/cdh4
这个路径下有很多的软件。</p>

<p>下载cdh4.6的几个包安装</p>

<p><code>bash
$ cd /home/software/
$ wget http://archive.cloudera.com/cdh4/cdh/4/hadoop-2.0.0-cdh4.6.0.tar.gz
$ sudo mkdir /usr/local/hadoop/
$ tar xzf hadoop-2.0.0-cdh4.6.0.tar.gz 
$ sudo mv hadoop-2.0.0-cdh4.6.0 /usr/local/
$ sudo mv /usr/local/hadoop-2.0.0-cdh4.6.0 /usr/local/hadoop
$ sudo chown -R hadoop:hadoop /usr/local/hadoop
</code>
创建存储临时文件temp、data和name节点数据的目录</p>

<p><code>bash
$ sudo mkdir /usr/local/hadoop/temp/ /usr/local/hadoop/data/ /usr/local/hadoop/name/ 
$ sudo chown -R hadoop:hadoop /usr/local/hadoop
</code></p>

<p>好了，准备工作终了</p>

<p>开始配置
配置/usr/local/hadoop/etc/hadoop/core-site.xml</p>

<p>```xml</p>
<configuration>
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://mdn3namenode1.net:9000</value> <!-- master域名或者master的ip -->
        </property>
        <property>
                <name>io.file.buffer.size</name>
                <value>131072</value>
        </property>
        <property>
                <name>hadoop.tmp.dir</name>
                <value>file:/usr/local/hadoop/temp</value>
                <description>Abase for other temporary directories.</description>
        </property>
        <property>
                <name>hadoop.proxyuser.hduser.hosts</name>
                <value>*</value>
        </property>
        <property>
                <name>hadoop.proxyuser.hduser.groups</name>
                <value>*</value>
        </property>
</configuration>
<p>```</p>

<p>配置/usr/local/hadoop/etc/hadoop/hdfs-site.xml</p>

<p>```xml</p>
<configuration>
        <property>
                <name>dfs.namenode.secondary.http-address</name>
                <value>mdn3namenode1.net:9001</value> <!-- master域名或者master的ip -->
        </property>
        <property>
                <name>dfs.namenode.name.dir</name>
        <value>file:/usr/local/hadoop/dfs/name</value>
        </property>
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>file:/usr/local/hadoop/dfs/data</value>
        </property>
        <property>
                <name>dfs.replication</name>
                <value>3</value>
        </property>
        <property>
                <name>dfs.webhdfs.enabled</name>
                <value>true</value>
        </property>
</configuration>
<p>```</p>

<p>配置/usr/local/hadoop/etc/hadoop/madpred-site.xml</p>

<p><code>bash
cp mapred-site.xml.template mapred-site.xml
</code></p>

<p>```xml</p>
<configuration>
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.address</name>
                <value>mdn3namenode1.net:10020</value> <!-- master域名或者master的ip -->
        </property>
        <property>
                <name>mapreduce.jobhistory.webapp.address</name>
                <value>mdn3namenode1.net:19888</value> <!-- master域名或者master的ip -->
        </property>
</configuration>
<p>```</p>

<p>配置/usr/local/hadoop/etc/hadoop/yarn-site.xml</p>

<p>```xml</p>
<configuration>
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce.shuffle</value>
        </property>
        <property>
                <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
                <value>org.apache.hadoop.mapred.ShuffleHandler</value>
        </property>
        <property>
                <name>yarn.resourcemanager.address</name>
                <value>mdn3namenode1.net:8032</value> <!-- master域名或者master的ip -->
        </property>
        <property>
                <name>yarn.resourcemanager.scheduler.address</name>
                <value>mdn3namenode1.net:8030</value> <!-- master域名或者master的ip -->
        </property>
        <property>
                <name>yarn.resourcemanager.resource-tracker.address</name>
                <value>mdn3namenode1.net:8031</value> <!-- master域名或者master的ip -->
        </property>
        <property>
                <name>yarn.resourcemanager.admin.address</name>
                <value>mdn3namenode1.net:8033</value> <!-- master域名或者master的ip -->
        </property>
        <property>
                <name>yarn.resourcemanager.webapp.address</name>
                <value>mdn3namenode1.net:8088</value> <!-- master域名或者master的ip -->
        </property>
</configuration>
<p>```</p>

<p>编辑slave的名字
直接讲slave的域名或者slave的ip按照一行一个的规则写进去</p>

<p><code>bash
mdn3datanode2.net
mdn3datanode3.net
</code></p>

<p>复制到各台机器上</p>

<p><code>bash
$ cd /usr/local/
$ sudo scp -dr hadoop@192.168.216.183:/usr/local/hadoop .
$ sudo chown -R hadoop:hadoop hadoop/
</code></p>

<p>格式化hdfs
在namenode上执行</p>

<p><code>bash
/usr/local/hadoop/bin/hadoop namenode -format
</code></p>

<h3 id="hbase">hbase的安装配置</h3>
<p>hbase依赖zookeeper，需要先去下载</p>

<p><code>bash
$ cd /home/software/
$ wget http://archive.cloudera.com/cdh4/cdh/4/zookeeper-3.4.5-cdh4.6.0.tar.gz
$ tar xzf zookeeper-3.4.5-cdh4.6.0.tar.gz
$ sudo mv zookeeper-3.4.5-cdh4.6.0 /usr/local/
$ sudo mv /usr/local/zookeeper-3.4.5-cdh4.6.0 /usr/local/zookeeper
$ sudo chown -R hadoop:hadoop /usr/local/zookeeper
$ sudo cp /usr/local/zookeeper/conf/zoo_sample.cfg /usr/local/zookeeper/conf/zoo.cfg
</code>
zookeeper准备完毕，可以继续安装hbase</p>

<p><code>bash
$ cd /home/software/
$ wget http://archive.cloudera.com/cdh4/cdh/4/hbase-0.94.15-cdh4.6.0.tar.gz
$ sudo mkdir /usr/local/hbase/
$ tar xzf hbase-0.94.15-cdh4.6.0.tar.gz
$ sudo mv hbase-0.94.15-cdh4.6.0 /usr/local/
$ sudo mv /usr/local/hbase-0.94.15-cdh4.6.0 /usr/local/hbase
$ sudo chown -R hadoop:hadoop /usr/local/hbase
</code></p>

<p>若干配置步骤
配置hbase-site.xml</p>

<p>```xml</p>
<configuration>
    <property>
        <name>hbase.rootdir</name>
        <value>hdfs://mdn3namenode1.net:9000/hbase</value>

    </property>
    <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
    </property>
    <property>
        <name>hbase.master</name>
        <value>mdn3datanode1.net:60000</value>
    </property>
    <property>
        <name>hbase.zookeeper.quorum</name>
        <value>mdn3datanode1.net</value>    <!-- 这里配置若干个zookeeper的服务器地址，需要是奇数个 -->
    </property>
</configuration>
<p>```</p>

<p>配置hbase-env.sh</p>

<p><code>bash
export HBASE_MANAGES_ZK=false
</code></p>

<p>不要hbase托管zookeeper</p>

<p>配置regionservers</p>

<p><code>bash
mdn3datanode2.net
mdn3datanode3.net
</code></p>

<p>启动hbase</p>

<p><code>bash
/usr/local/hbase/bin/start-hbase.sh
/usr/local/hbase/bin/hbase-daemons.sh start thrift
</code>
hbase启动完成.</p>

<h3 id="hbase-1">配置hbase可能碰到几个问题的说明：</h3>
<p>1) 报错
` ERROR client.HConnectionManager$HConnectionImplementation: Check the value configured in ‘zookeeper.znode.parent’ `</p>

<p>是需要把/etc/hosts中的127.0.0.1注释掉，否则zookeeper还会出现
最后的hosts我这里是这样</p>

<p><code>bash
[hadoop@localhost conf]$ more /etc/hosts
#127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.216.183 mdn3namenode1.net mdn3datanode1.net
192.168.216.184 mdn3namenode2.net mdn3datanode2.net
192.168.216.185 mdn3datanode3.net mdn3nfsserver.net
</code></p>

<p>2) 在运行/usr/local/hbase/bin/hbase shell的时候出现了
` WARN conf.Configuration: hadoop.native.lib is deprecated. Instead, use io.native.lib.available `</p>

<p>3) ` java.net.ConnectException: Connection refused `
这是要求hadoop中的slaves配置和hbase的regionservers要一致。</p>

<h3 id="hive">hive的安装</h3>

<p><code>bash
cd /home/software
wget http://archive.cloudera.com/cdh4/cdh/4/hive-0.10.0-cdh4.6.0.tar.gz
tar xzf hive-0.10.0-cdh4.6.0.tar.gz
sudo mv hive-0.10.0-cdh4.6.0 /usr/local/
sudo mv /usr/local/hive-0.10.0-cdh4.6.0 /usr/local/hive
chown -R hadoop:hadoop /usr/local/hive
</code></p>

<h3 id="hive-1">hive的配置</h3>
<p>在~/.bashrc中加入</p>

<p><code>bash
export HIVE_HOME=/usr/local/hive
export HIVE_CONF_DIR=$HIVE_HOME/conf
export HIVE_LIB=$HIVE_HOME/lib
export PATH=$PATH:$JAVA_HOME/bin:$ZOOKEEPER_HOME:$HIVE_HOME
</code></p>

<p>在conf/hive-site.xml中</p>

<p>```xml</p>
<configuration>
<property>
  <name>hive.metastore.local</name>
  <value>true</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://localhost:3306/hive</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hive</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>hive</value>
</property>
<property>
  <name>datanucleus.fixedDatastore</name>
  <value>false</value>
</property>

</configuration>
<p>```</p>

<p>这里要安装mysql作为元数据服务器，参考这篇 http://evoupsight.com/blog/2014/02/17/hadoop0-dot-20-dot-2-plus-hive0-dot-7/</p>

<p>然后/bin/hive后，成功进入shell</p>

<p><code>bash
&gt; create table test (key string);
</code></p>

<p>如果遇到下面的报错
` FAILED: Error in metadata: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient `</p>

<p><code> FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask </code>
建表错误
开始以为hive没有访问mysql的权限,以root用户登录mysql然后赋予hive用户权限：</p>

<p><code>sql
grant all privileges on *.* to hive@localhost identified by 'hive';
grant all privileges on *.* to hive@192.168.216.183 identified by 'hive';
</code></p>

<p>发现问题依旧</p>

<p>其实是要在hive-site.xml中把</p>

<p>```xml</p>
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://localhost:3306/hive</value>
</property>
<p>```</p>

<p>改成</p>

<p>```xml</p>
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://192.168.216.183:3306/hive</value>
</property>
<p>```</p>

<p>问题依旧，打开hive的调试模式</p>

<p><code>bash
bin/hive -hiveconf hive.root.logger=DEBUG,console
</code></p>

<p><code> 14/05/08 17:35:53 WARN conf.HiveConf: DEPRECATED: Configuration property hive.metastore.local no longer has any effect. </code>
` Make sure to provide a valid value for hive.metastore.uris if you are connecting to a remote metastore `</p>

<p>在配置文件里删除hive.metastore.local属性。</p>

<p>最后查得原因是没有安装mysql驱动，只要把mysql-connector-java-5.1.22-bin.jar放到lib下就可以了</p>

<p>然后</p>

<p>```bash
hive&gt; create table test (key string);
OK
Time taken: 42.259 seconds</p>

<p>hive&gt; show tables;
OK
test
Time taken: 0.279 seconds
```</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[hbase连zookeeper瞬断]]></title>
    <link href="http://evoupsight.com/blog/2013/12/31/hbase-access-zookeeper-fail-too-many-connections-form-ip/"/>
    <updated>2013-12-31T11:48:00+08:00</updated>
    <id>http://evoupsight.com/blog/2013/12/31/hbase-access-zookeeper-fail-too-many-connections-form-ip</id>
    <content type="html"><![CDATA[<h3 id="section">问题</h3>

<p>今天修理hbase问题的时候发现，监控的60010端口的master.jsp就是无法显示，进入log查看发现zookeeper连上了之后马上就断开。</p>

<p><code> [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn$Factory@247] </code> 
` - Too many connections from /10.10.8.136 - max is 10 `</p>

<p>这种情况在telnet测试中被证实，一连上也是瞬间脱离与服务器的连接。</p>

<!-- more -->

<h3 id="section-1">解决</h3>

<p>其实需要在zoo.cfg中加入maxClientCnxns=300，加完以后需要重启。问题解决。</p>

<h3 id="section-2">原因</h3>

<p>我们线上有24台节点，但是这个参数竟然是使用默认的10，导致更多的客户端连上了zookeeper导致namenode的自带管理页无法连接到zookeeper，进而无法显示该页面。</p>

<p>如何监控zookeeper的其他指标，这里列出zoo.cfg的配置文件</p>

<p>```bash
dataDir = 数据存放路径</p>

<p>dataLogDir = 日志存放路径</p>

<p>clientPort = 客户端连接端口</p>

<p>clientPortAddress</p>

<p>tickTime= 整型 不能为0</p>

<p>maxClientCnxns= 整型 最大客户端连接数</p>

<p>minSessionTimeout= 整型</p>

<p>maxSessionTimeout= 整型</p>

<p>initLimit = 整型</p>

<p>syncLimit = 整型</p>

<p>electionAlg = 整型</p>

<table>
  <tbody>
    <tr>
      <td>peerType = observer</td>
      <td>participant</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>server. sid= host:port</td>
      <td>host:port:port</td>
      <td>host:port:port:type (type值 observer</td>
      <td>participant)</td>
    </tr>
  </tbody>
</table>

<p>group.gid = sid:sid (一个ID， 值是多个sid, 中间以:分割， 一个sid只能属于一个gid)</p>

<p>weight.sid=整型
```</p>

<p>可以看出还有至少2个参数是需要考虑的minSessionTimeout和maxSessionTimeout需要调优，得用JMX监控一段时间得出结论了。</p>

<p>同样的发现thrift也存在类似一连就断开的问题，下篇博文再作分析。</p>

<h3 id="section-3">总结</h3>
<p>这个案例告诉我不要盲目认为按照默认参数配置就没问题了，那是给小批量测试用的，需要根据实际情况采取相应配置。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HBASE0.90.6完全分布式搭建(VMware版)]]></title>
    <link href="http://evoupsight.com/blog/2013/11/04/hbase-full-distributed-case/"/>
    <updated>2013-11-04T16:28:00+08:00</updated>
    <id>http://evoupsight.com/blog/2013/11/04/hbase-full-distributed-case</id>
    <content type="html"><![CDATA[<p>接上一篇<a href="http://evoupsight.com/blog/2013/11/04/hadoop-full-distributed-case/">《HADOOP完全分布式搭建(VMware版)》</a></p>

<p>参考 http://www.cnblogs.com/flyoung2008/archive/2011/12/02/2272761.html</p>

<!-- more -->

<p><code>bash
cd /u01/app
tar xzf hbase-0.90.6.tgz
ln -s hbase-0.90.6 hbase
cd hbase/conf
</code></p>

<p>编辑hbase-env.sh
指定java的路径,默认HBASE_MANAGES_ZK=true，这个代表采用hbase来托管zookeeper，这样重启hbase会连带重启zookeeper</p>

<p><code>bash
export HBASE_OPTS="-ea -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode"
export JAVA_HOME=/usr/java/jdk1.6.0_29
export HBASE_MANAGES_ZK=true
</code></p>

<p>注意HADOOP_HOME和HBASE_HOME已经在~/.profile中指定，不需要再设置了。</p>

<hr />

<p>(补充：2014-2-19 发现这么写还不能加载，放到~/.bashrc中才对，见下)</p>

<p><code>bash
export HADOOP_HOME=/u01/app/hadoop
</code></p>

<hr />

<p>编辑hbase-site.xml</p>

<p>```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
/**
 * Copyright 2010 The Apache Software Foundation
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
--></p>
<configuration>
    <property>
        <name>hbase.rootdir</name>
        <value>hdfs://mdn2.net:9024/hbase</value>
    </property>
    <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
    </property>
    <property>
        <name>hbase.master</name>
        <value>mdn2.net:60000</value>
    </property>
    <property>
        <name>hbase.zookeeper.quorum</name>
        <value>mdn2.net,mdn2datanode1.net,mdn2datanode2.net</value>
    </property>
</configuration>
<p>```</p>

<p>注意点：
 1.其中首先需要注意hdfs://mdn2.net:9024/hbase这里，必须与你的Hadoop集群的core-site.xml文件配置保持完全一致才行，如果你Hadoop的hdfs使用了其它端口，请在这里也修改。再者就是Hbase该项并不识别机器IP，只能使用机器hostname才可行，即若使用IP是会抛出java错误。
 2.hbase.zookeeper.quorum 的个数必须是奇数，否则也可以启动hbase，只不过用thrift接口时无法使用，会类似这样的错<code>INFO org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation:</code> <br /> <code>ZooKeeper available but no active master location found</code> </p>

<p>修改regionservers文件（同hadoop的slaves文件）</p>

<p><code>bash
mdn2datanode1.net
mdn2datanode2.net
</code></p>

<p>然后分发到各点，就可以启动了。</p>

<p><code>bash
bin/start hbase
bin/hbase shell
</code></p>

<p>报错
ERROR: org.apache.hadoop.hbase.ZooKeeperConnectionException: HBase is able to connect to ZooKeeper but the connection closes immediately. This could be a sign that the server has too many connections (30 is the default). Consider inspecting your ZK server logs for that error and then make sure you are reusing HBaseConfiguration as often as you can. See HTable’s javadoc for more information.
看来不要使用hbase托管的zookeeper转而再装一个试试。</p>

<p><code>bash
export HBASE_MANAGES_ZK=true
</code></p>

<p>改为</p>

<p><code>bash
export HBASE_MANAGES_ZK=false
</code></p>

<p>编辑~/.profile,加入关于zk环境变量的设置</p>

<p><code>bash
export ZOOKEEPER_HOME="/u01/app/zookeeper/"
PATH=$ZOOKEEPER_HOME/bin:$PATH
export PATH
</code></p>

<p><code>bash
cd /u01/app
wget http://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.4.4/zookeeper-3.4.4.tar.gz
tar xzf zookeeper-3.4.4.tar.gz
ln -s zookeeper-3.4.4 zookeeper
cd /u01/app/zookeeper/conf
cp zoo_sample.cfg zoo.cfg
cd ../bin
./zkServer.sh start
</code></p>

<p>最后重启整个hadoop/hbase搞定，jps看下跑的进程。收工。</p>

<p><img src="/images/evoup/hbase_vmware.png" alt="Alt text" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[freebsd下hbase安装]]></title>
    <link href="http://evoupsight.com/blog/2011/07/30/freebsd-install-hbase/"/>
    <updated>2011-07-30T21:51:00+08:00</updated>
    <id>http://evoupsight.com/blog/2011/07/30/freebsd-install-hbase</id>
    <content type="html"><![CDATA[<p>安装JDK补遗,port中该文件已经无法获取。关键是下载到diablo-caffe-freebsd7-i386-1.6.0_07-b02.tar.bz</p>

<p>http://www.freebsdfoundation.org/cgi-bin/download?download=diablo-caffe-freebsd7-amd64-1.6.0_07-b02.tar.bz2</p>

<!-- more -->

<p>下完就放到/usr/port/distfile</p>

<p>进到/usr/ports/java/diablo-jdk16之后也要把timezone那个选取消。</p>

<p>再装jdk16/usr/port/java/jdk16</p>

<p><code>bash
sudo axel http://www.java.net/download/jdk6/6u3/promoted/b05/jdk-6u3-fcs-src-b05-jrl-24_sep_2007.jar 
sudo mv jdk-6u3-fcs-src-b05-jrl-24_sep_2007.jar php-5.3.8.tar.bz2 /usr/ports/distfiles/
</code></p>

<p>装apache-ant，自动的，如果不行cd /usr/port/devel/apache-ant/ sudo make install clean</p>

<p>而且要有足够的SWAP空间！</p>

<p><code>bash
setenv JAVA_HOME /usr/local/jdk1.6.0/
</code>
PATH我没有设置</p>

<p>然后是hbase的安装，拿到权威手册翻一翻就能知道</p>

<p>单个节点的启动方法</p>

<p><code>bash
sudo ./bin/start-hbase.sh
sudo ./bin/hbase-daemon.sh start thrift
</code>
hshell的进入</p>

<p><code>bash
sudo ./bin/hbase shell
</code></p>

]]></content>
  </entry>
  
</feed>
